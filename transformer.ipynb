{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69fae65d",
   "metadata": {},
   "source": [
    "# Generative AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c3ea518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from datasets import load_dataset\n",
    "# check for gpu\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d294c557",
   "metadata": {},
   "source": [
    "## Making the data ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dfa0617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'translation': {'cs': 'Následný postup na základě usnesení Parlamentu: viz zápis', 'en': \"Action taken on Parliament's resolutions: see Minutes\"}}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the dataset (ensure the right dataset is loaded)\n",
    "dataset = load_dataset(\"wmt14\", \"cs-en\", cache_dir = \"./data_cache\")\n",
    "\n",
    "# Define the tokenizer (using a pretrained model like T5)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")  # Example with T5\n",
    "\n",
    "\n",
    "\n",
    "# Only use first 10k samples\n",
    "small_dataset = dataset['train'].select(range(10_000))\n",
    "\n",
    "# Check the structure of the first example in the train dataset\n",
    "print(small_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f56ab65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['translation'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58730a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [6776, 1026, 30, 12876, 31, 7, 3161, 7, 10, 217, 13687, 7, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [445, 2975, 7, 1361, 29, 2, 442, 413, 3, 29, 9, 3, 172, 2975, 8142, 26, 2, 178, 1496, 35, 2, 13636, 76, 10, 3, 7302, 3, 172, 2975, 102, 159, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "max_length = 64\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize source (EN) and target (DE) from the dictionary keys\n",
    "    sources = examples[\"translation\"]  # This is a list of dictionaries\n",
    "    source_texts = [source[\"en\"] for source in sources]  # Extract English sentences\n",
    "    target_texts = [source[\"cs\"] for source in sources]  # Extract German sentences\n",
    "    \n",
    "    # Tokenize the source texts (for the encoder)\n",
    "    model_inputs = tokenizer(\n",
    "        source_texts,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"  # Ensure that the output is in tensor format\n",
    "    )\n",
    "    \n",
    "    # Tokenize the target texts (for the decoder)\n",
    "    labels = tokenizer(\n",
    "        target_texts,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Add labels to the model inputs dictionary\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# Apply tokenizer across the entire dataset\n",
    "tokenized_dataset = small_dataset.map(tokenize_function, batched=True, remove_columns=[\"translation\"])\n",
    "\n",
    "# Verify the result\n",
    "print(tokenized_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c1c3e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set format to PyTorch tensors\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"labels\"])\n",
    "\n",
    "# Create PyTorch DataLoader\n",
    "train_dataloader = DataLoader(tokenized_dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73683487",
   "metadata": {},
   "source": [
    "## Creating architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90cb5849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.nn import functional as F\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Q, K, V: (B, h, L, d_k)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(Q.size(-1))  # (B, h, L, L)\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attn = F.softmax(scores, dim=-1)  # (B, h, L, L)\n",
    "        output = torch.matmul(attn, V)  # (B, h, L, d_k)\n",
    "\n",
    "        return output, attn  # return both output and attention weights (as in paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14060f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "attention = ScaledDotProductAttention()\n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "dk = 8\n",
    "dv = 8\n",
    "\n",
    "Q = torch.randn(batch_size, seq_len, dk)\n",
    "K = torch.randn(batch_size, seq_len, dk)\n",
    "V = torch.randn(batch_size, seq_len, dv)\n",
    "\n",
    "output, attn = attention(Q, K, V)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84753cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        # Linear projections for Q, K, V\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Final output projection\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.attention = ScaledDotProductAttention()\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        B, L, d_model = Q.size()\n",
    "\n",
    "        # Linear projections\n",
    "        Q = self.W_q(Q).view(B, L, self.num_heads, self.head_dim).transpose(1, 2)  # (B, h, L, d_k)\n",
    "        K = self.W_k(K).view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.W_v(V).view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Apply attention\n",
    "        attn_output, attn_weights = self.attention(Q, K, V, mask)  # (B, h, L, d_k)\n",
    "\n",
    "        # Concatenate heads\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(B, L, d_model)  # (B, L, d_model)\n",
    "\n",
    "        # Final linear projection\n",
    "        output = self.W_o(attn_output)  # (B, L, d_model)\n",
    "\n",
    "        return output, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92993beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 10, 512])\n",
      "Attention weights shape: torch.Size([2, 8, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "Q = torch.randn(2, 10, 512)  # batch_size=2, seq_len=10, d_model=512\n",
    "K = torch.randn(2, 10, 512)\n",
    "V = torch.randn(2, 10, 512)\n",
    "\n",
    "output, attn_weights = mha(Q, K, V)\n",
    "print(\"Output shape:\", output.shape)       # (2, 10, 512)\n",
    "print(\"Attention weights shape:\", attn_weights.shape)  # (2, 8, 10, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d7f33e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        x = self.relu(self.linear1(x))\n",
    "        return self.linear2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c7e9469",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len=64):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create a (max_len, d_model) matrix\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position* div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe) # save as buffer (not a paramter)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        seq_len = x.size(1)\n",
    "        # add positional_encoding\n",
    "        x = x + self.pe[:, :seq_len, :] # type: ignore\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75479a1d",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cd693a",
   "metadata": {},
   "source": [
    "### Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52de76c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCell(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff = PositionWiseFeedForward(d_model, d_ff)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask=None) -> torch.Tensor:\n",
    "        # x : (batch_size, seq_len, d_model)\n",
    "        # mask : (batch_size, 1, 1, seq_len), optional for padding masking\n",
    "\n",
    "        # Multi-Head Self Attention\n",
    "        attn_output, _ = self.mha(x, x, x, mask=mask)\n",
    "        \n",
    "        # Add residual connection and layer normalization\n",
    "        x = self.layer_norm1(x + attn_output)\n",
    "\n",
    "        # Feed-forward network\n",
    "        ff_output = self.ff(x)\n",
    "\n",
    "        # Add residual connection and layer normalization\n",
    "        x = self.layer_norm2(x + ff_output)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54240294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 64, 512])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((32, 64, 512))\n",
    "encoder_cell = EncoderCell(512, 8, 2048)\n",
    "\n",
    "encoder_cell(x).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7c8af5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderCell(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff:int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm3 = nn.LayerNorm(d_model)\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff = PositionWiseFeedForward(d_model, d_ff)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, enc_output: torch.Tensor, tgt_mask=None, src_mask=None) -> torch.Tensor:\n",
    "        # x : (batch_size, tgt_seq_len, d_model)\n",
    "        # enc_output: (batch_size, src_seq_len, d_model)\n",
    "\n",
    "        # Self-attention with masking (prevent future token attention in decoder)\n",
    "        attn_output1, _ = self.self_attn(x, x, x, mask=tgt_mask)\n",
    "        x = self.layer_norm1(x + attn_output1)  # Residual + LayerNorm\n",
    "\n",
    "        # Encoder-Decoder attention (decoder attends to encoder output)\n",
    "        attn_output2, _ = self.enc_dec_attn(x, enc_output, enc_output, mask=src_mask)\n",
    "        x = self.layer_norm2(x + attn_output2)  # Residual + LayerNorm\n",
    "\n",
    "        # Feed forward\n",
    "        ff_output = self.ff(x)\n",
    "        x = self.layer_norm3(x + ff_output)  # Residual + LayerNorm\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd1954e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff, vocab_size, max_len=512):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = PositionalEncoding(d_model, max_len)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderCell(d_model, num_heads, d_ff)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        x = self.token_embed(src)  # (batch_size, src_seq_len, d_model)\n",
    "        x = self.pos_embed(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_mask)\n",
    "\n",
    "        x = self.norm(x)  # Final norm after last layer\n",
    "        return x  # (batch_size, src_seq_len, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21878682",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff, vocab_size, max_len=512):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = PositionalEncoding(d_model, max_len)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderCell(d_model, num_heads, d_ff)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, tgt, enc_output, tgt_mask=None, src_mask=None):\n",
    "        x = self.token_embed(tgt)  # (batch_size, tgt_seq_len, d_model)\n",
    "        x = self.pos_embed(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_output, tgt_mask, src_mask)\n",
    "\n",
    "        x = self.norm(x)  # Final norm\n",
    "        return x  # (batch_size, tgt_seq_len, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b22787b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff, src_vocab_size, tgt_vocab_size, max_len=512):\n",
    "        super().__init__()\n",
    "\n",
    "        \n",
    "        # Encoder and Decoder\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, d_ff, src_vocab_size, max_len)\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, d_ff, tgt_vocab_size, max_len)\n",
    "\n",
    "        # Final linear layer projects decoder output to vocab logits\n",
    "        self.output_linear = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        \n",
    "        # Forward pass through the encoder and decoder\n",
    "        enc_output = self.encoder(src, src_mask)\n",
    "        dec_output = self.decoder(tgt, enc_output, tgt_mask, src_mask)\n",
    "\n",
    "        # Project decoder output to logits\n",
    "        logits = self.output_linear(dec_output)  # (batch_size, tgt_seq_len, tgt_vocab_size)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50c063b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq, pad_token_id=0):\n",
    "    # seq: (batch_size, seq_len)\n",
    "    return (seq == pad_token_id).unsqueeze(1).unsqueeze(2)\n",
    "    # output shape: (batch_size, 1, 1, seq_len) ➔ broadcastable for attention\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = torch.triu(torch.ones((size, size)), diagonal=1)\n",
    "    return mask == 1  # boolean mask\n",
    "\n",
    "def create_decoder_mask(tgt_seq, pad_token_id=0):\n",
    "    tgt_seq_len = tgt_seq.size(1)\n",
    "\n",
    "    look_ahead_mask = create_look_ahead_mask(tgt_seq_len).to(tgt_seq.device)  # (tgt_seq_len, tgt_seq_len)\n",
    "    padding_mask = create_padding_mask(tgt_seq, pad_token_id)  # (batch_size, 1, 1, tgt_seq_len)\n",
    "\n",
    "    combined_mask = look_ahead_mask.unsqueeze(0).unsqueeze(0) | padding_mask\n",
    "    return combined_mask  # (batch_size, 1, tgt_seq_len, tgt_seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6cc2c761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_right(tgt_batch, pad_token_id=0, bos_token_id=2):\n",
    "    shifted = torch.full_like(tgt_batch, pad_token_id)\n",
    "    shifted[:, 1:] = tgt_batch[:, :-1]\n",
    "    shifted[:, 0] = bos_token_id  # Start with BOS token\n",
    "    return shifted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "588fe231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "def shift_right(tgt_batch, pad_token_id=0, bos_token_id=2):\n",
    "    shifted = torch.full_like(tgt_batch, pad_token_id)\n",
    "    shifted[:, 1:] = tgt_batch[:, :-1]\n",
    "    shifted[:, 0] = bos_token_id  # Start with BOS token\n",
    "    return shifted\n",
    "\n",
    "def train_model(model, train_dataloader, num_epochs=10, lr=3e-4, device='cuda', pad_token_id=0, bos_token_id=2):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for batch in tqdm(train_dataloader, desc=\"Training\", leave=False):\n",
    "            src_batch = batch['input_ids'].to(device)  # shape: (B, src_len)\n",
    "            tgt_batch = batch['labels'].to(device)     # shape: (B, tgt_len)\n",
    "\n",
    "            # Shift target right for decoder input\n",
    "            tgt_input = shift_right(tgt_batch, pad_token_id, bos_token_id).to(device)\n",
    "\n",
    "            # Create masks\n",
    "            src_mask = create_padding_mask(src_batch, pad_token_id).to(device)  # (B, 1, 1, src_len)\n",
    "            tgt_mask = create_decoder_mask(tgt_input, pad_token_id).to(device)  # (B, 1, tgt_len, tgt_len)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(src_batch, tgt_input, src_mask, tgt_mask)  # (B, tgt_len, vocab_size)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)),  # (B * tgt_len, vocab_size)\n",
    "                tgt_batch.view(-1),                # (B * tgt_len)\n",
    "                ignore_index=pad_token_id\n",
    "            )\n",
    "\n",
    "            # Backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss / len(train_dataloader):.4f}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3cbb3924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "num_layers = 6\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "src_vocab_size = 32000  # depends on your tokenizer\n",
    "tgt_vocab_size = 32000\n",
    "max_len = 512\n",
    "\n",
    "# 1. Instantiate model\n",
    "model = Transformer(num_layers, d_model, num_heads, d_ff, src_vocab_size, tgt_vocab_size, max_len)\n",
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b368465b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:57<08:33, 57.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.4747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [01:53<07:35, 56.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 0.0807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Assuming your model is already defined (e.g., transformer_model)\n",
    "trained_model = train_model(model, train_dataloader, num_epochs=10, lr=3e-4, device='cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029a2ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
